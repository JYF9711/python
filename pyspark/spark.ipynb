{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "dat=[1,2,3,4,5]\n",
    "sc=SparkContext(\"local\",\"wordcount\")\n",
    "disDat=sc.parallelize(dat)\n",
    "disDat.collect()\n",
    "disDat=sc.parallelize(dat,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "textFile = sc.textFile(\"file:///opt/text/python/pyspark/data.txt\")\n",
    "wordCount = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b)\n",
    "wordCount.collect()\n",
    "wordCount.foreach(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD综合应用 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "随即生成年龄\n",
    "agegenerate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "for i in range(10):\n",
    "    num=rd.random()*100\n",
    "    str1=str(i)+ \" \"+str(int(num))\n",
    "    print(str1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "新建age.txt,内容为以上结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD操作计算平均年龄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由年龄文件生成人rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "rdd1=sc.textFile(\"age.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD转换（年龄，1）形式的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda line : line.split(\" \")).map(lambda x:(int(x[1]),1))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.keys().reduce(lambda x,y : x+y)/rdd2.values().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"spark\",2),(\"hadoop\",6),(\"hadoop\",4),(\"spark\",6)])\n",
    "rdd.mapValues(lambda x : (x,1)).reduceByKey(lambda x,y : (x[0]+y[0],x[1] + y[1])).mapValues(lambda x : (x[0] / x[1])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([\"b\",\"a\",\"c\"])\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()\n",
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验四---SparkDataFram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "book.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "将book的DataFrame转化RDD操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "book_rdd.take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "创建dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df=spark.createDataFrame(book_rdd)\n",
    "book_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#指定列输出\n",
    "book_df.select('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#查看方法和属性\n",
    "dir(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "print(book_pds.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_pds=book_pds.toPandas()\n",
    "print(book_pds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "#转化Rdd\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "#创建dataFrame\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "# book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()\n",
    "#pandas\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "book_pds=book_pds.toPandas()\n",
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "\n",
    "#查看前五行数据\n",
    "# book.take(5)\n",
    "\n",
    "#查看类型\n",
    "# type(book)\n",
    "\n",
    "#打印表头数据\n",
    "# book.printSchema()\n",
    "\n",
    "#将book的DataFrame转化为RRD\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda  x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],url=x[5]))\n",
    "# book_rdd.take(5)\n",
    "#创建dataFrame\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "#查询前五行数据\n",
    "# book_df.show(5)\n",
    "#查询前50行\n",
    "#打印schema\n",
    "# book_df.printSchema()\n",
    "#创建临时表 \n",
    "book_df.registerTempTable(\"tb_book\")\n",
    "#使用sql语句查询 \n",
    "spark.sql(\"select * from tb_book\").show(15)\n",
    "#模糊查询书名包含“微积分”的书 \n",
    "spark.sql(\"select * from tb_book where name like '%微积分%'\").show()\n",
    "#输出图书的前10行的name和price字段信息\n",
    "spark.sql(\"select name,price from tb_book \").show(10)\n",
    "#统计书名包含“微积分”的书的数量 \n",
    "spark.sql(\"select count(*) from tb_book where name like '%微积分%'\").show()\n",
    "#查询评分大于9的图书 \n",
    "spark.sql('select * from tb_book where rating>9').show(10)\n",
    "#计算所有书名包含“微积分”的评分平均值 \n",
    "spark.sql(\"select avg(rating) rating_avg from tb_book where name like '%微积 分%'\")\n",
    "#把书目按照评分从高到低进行排列 \n",
    "spark.sql(\"select id,name,rating from tb_book order by rating desc\").show(15)\n",
    "#把图书按照出版社进行分组 \n",
    "spark.sql(\"select publish,count(name) as cnt from tb_book group by publish order by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验六"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# sc=SparkContext(\"local\",\"appName\")\n",
    "sc=SparkContext(\"local\",\"PythonStreamingHdfSWordCount\")\n",
    "scc=StreamingContext(sc,5)\n",
    "\n",
    "type(scc)\n",
    "\n",
    "# lines=scc.textFileStream(\"file:///opt/text/python/listen\")\n",
    "# lines=scc.textFileStream(\"hdfs://localhost:8020/listen\")\n",
    "lines=scc.socketTextStream('localhost',9999)\n",
    "\n",
    "\n",
    "\n",
    "counts = lines.flatMap(lambda line:line.split(\" \")).map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "counts.pprint()\n",
    "scc.start();scc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验七"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#读取用户\n",
    "user_df=spark.read.text('file:///opt/text/python/ml-100k/u.user')\n",
    "user_df.show(10)\n",
    "\n",
    "#为用户数据添加 schem\n",
    "from pyspark import Row\n",
    "user_rdd = user_df.rdd.map(lambda x:x[0].split('|')).map(lambda x:Row(id=x[0],age=x[1],sex=x[2],job=x[3],code=x[4]))\n",
    "user_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建用户 dataframe\n",
    "user_df=spark.createDataFrame(user_rdd)\n",
    "\n",
    "#注册临时表\n",
    "user_df.registerTempTable(\"user\")\n",
    "spark.sql('select id,age,sex,job,code from user').show()\n",
    "\n",
    "\n",
    "#读取评分数据\n",
    "rating_df = spark.read.text('file:///opt/text/python/ml-100k/u.data')\n",
    "rating_df.show(5)\n",
    "\n",
    "\n",
    "#为评分数据添加 schema\n",
    "rating_rdd = rating_df.rdd.map(lambda x:x[0].split()).\\\n",
    "    map(lambda x: Row(user_id=x[0], film_id=x[1], rating=x[2], time=x[3]))\n",
    "rating_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建评分 dataframe\n",
    "rating_df = spark.createDataFrame(rating_rdd)\n",
    "\n",
    "#注册评分临时表\n",
    "rating_df.registerTempTable('rating')\n",
    "spark.sql(\"select user_id,film_id,rating,time from rating\").show()\n",
    "\n",
    "\n",
    "#读取电影信息\n",
    "film_df = spark.read.text('file:///opt/text/python/ml-100k/u.item')\n",
    "film_df.show()\n",
    "\n",
    "\n",
    "#为电影数据添加 schema\n",
    "film_rdd = film_df.rdd.map(lambda x:x[0].split('|')).map(lambda x:Row(id=x[0],title=x[1],post_time=x[2],url=x[4]))\n",
    "film_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建电影 dataframe\n",
    "film_df = spark.createDataFrame(film_rdd)\n",
    "#注册电影临时表\n",
    "film_df.registerTempTable('film')\n",
    "spark.sql('select * from film').show()\n",
    "\n",
    "#导入 Spark 机器学习包：协同过滤算法\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "#ALS.trainImplicit 中第一个参数 RDD 是固定格式（用户编号，电影编号，评分值）\n",
    "rdd = rating_rdd.map(lambda x: (x[3], x[0], x[1]))\n",
    "rdd.take(5)\n",
    "\n",
    "#训练模型\n",
    "model = ALS.train(rdd, 50, 10, 0.01)\n",
    "#使用模型进行推荐，参数为用户 id，推荐个数\n",
    "user_film = model.recommendProducts(881250949, 3)\n",
    "print(user_film)\n",
    "\n",
    "#显示推荐电影名称\n",
    "type(user_film)\n",
    "for film in user_film:\n",
    "    spark.sql(\"select id ,title,post_time,url , \"+str(film[2])+\" as similar from film where id=\"+str(film[1])).show()\n",
    "\n",
    "\n",
    "#查看用户看过的电影\n",
    "spark.sql(\"select a.user_id,b.title from rating a,film b where a.film_id=b.id and a.user_id=112\").show()\n",
    "\n",
    "\n",
    "#保存模型\n",
    "import os\n",
    "import shutil\n",
    "save_path = '/opt/text/python/film_model'\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "model.save(sc, save_path)\n",
    "print('model saved!')\n",
    "#调用模型\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "model = MatrixFactorizationModel.load(sc, save_path)\n",
    "result = model.recommendProducts(881250949, 3)\n",
    "for film in user_film:\n",
    "    spark.sql(\"select id , title,post_time,url , \"+str(film[2])+\" as similar from film where id=\"+str(film[1])).show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验八 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([104.01762438,  30.65643936]), array([103.6522621 ,  30.89418873]), array([104.08803516,  30.64616188])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(\"local\",\"texi\")\n",
    "taxi_data=sc.textFile('file:///opt/text/python/taxi.csv')\n",
    "taxi_rdd=taxi_data.map(lambda line:line.split(','))\n",
    "from  pyspark.ml.linalg  import  Vectors\n",
    "taxi_row=taxi_rdd.map(lambda x: (Vectors.dense (x[2],x[1]), ))\n",
    "sqlsc=SparkSession.builder.getOrCreate()\n",
    "taxi_df=sqlsc.createDataFrame(taxi_row,[\"features\"])\n",
    "from  pyspark.ml.clustering  import  KMeans\n",
    "kmeans=KMeans(k=3, seed=1)  #聚成3类\n",
    "model=kmeans.fit(taxi_df)    #注意，传入的DataFrame是矢量名称为features的集合\n",
    "centers=model.clusterCenters()  #产生聚类集合\n",
    "print(centers)\n",
    "# Kmeans方法中包含啊若干个参数，k为分簇个数，seed为种子点，上方为核心代码。\n",
    "# 代码解析如下：\n",
    "# 第一行：从pyspark.ml.clustering 中导入KMeans。\n",
    "# 第二行：创建kmeans实例，k为分簇个数，seed为种子点。\n",
    "# 第三行：调用kmeans.fit方法训练形成模型\n",
    "# 第四行：产生聚类集合\n",
    "# 第五行：输出集合\n",
    "# 聚类的结果是一系列的点集，这些点集也就是出租车聚集的地区，上述代码将数据聚类成3类，如下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
