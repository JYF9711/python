{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "textFile = sc.textFile(\"file:///opt/text/python/pyspark/data.txt\")\n",
    "wordCount = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b)\n",
    "wordCount.collect()\n",
    "wordCount.foreach(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD综合应用 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "随即生成年龄\n",
    "agegenerate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "for i in range(10):\n",
    "    num=rd.random()*100\n",
    "    str1=str(i)+ \" \"+str(int(num))\n",
    "    print(str1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "新建age.txt,内容为以上结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD操作计算平均年龄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由年龄文件生成人rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "rdd1=sc.textFile(\"age.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD转换（年龄，1）形式的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda line : line.split(\" \")).map(lambda x:(int(x[1]),1))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.keys().reduce(lambda x,y : x+y)/rdd2.values().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"spark\",2),(\"hadoop\",6),(\"hadoop\",4),(\"spark\",6)])\n",
    "rdd.mapValues(lambda x : (x,1)).reduceByKey(lambda x,y : (x[0]+y[0],x[1] + y[1])).mapValues(lambda x : (x[0] / x[1])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([\"b\",\"a\",\"c\"])\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()\n",
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验四---SparkDataFram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "book.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "将book的DataFrame转化RDD操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "book_rdd.take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "创建dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df=spark.createDataFrame(book_rdd)\n",
    "book_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#指定列输出\n",
    "book_df.select('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#查看方法和属性\n",
    "dir(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "print(book_pds.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_pds=book_pds.toPandas()\n",
    "print(book_pds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "#转化Rdd\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "#创建dataFrame\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "# book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()\n",
    "#pandas\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "book_pds=book_pds.toPandas()\n",
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-1-998cf787c82e>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-998cf787c82e>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda \\ x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],url=x[5]))\u001b[0m\n\u001b[0m                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "#rdd\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda \\ x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],url=x[5]))\n",
    "#dataframe\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
