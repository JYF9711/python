{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "dat=[1,2,3,4,5]\n",
    "sc=SparkContext(\"local\",\"wordcount\")\n",
    "disDat=sc.parallelize(dat)\n",
    "disDat.collect()\n",
    "disDat=sc.parallelize(dat,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "textFile = sc.textFile(\"file:///opt/text/python/pyspark/data.txt\")\n",
    "wordCount = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b)\n",
    "wordCount.collect()\n",
    "wordCount.foreach(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD综合应用 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "随即生成年龄\n",
    "agegenerate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "for i in range(10):\n",
    "    num=rd.random()*100\n",
    "    str1=str(i)+ \" \"+str(int(num))\n",
    "    print(str1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "新建age.txt,内容为以上结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD操作计算平均年龄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由年龄文件生成人rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'mywordcount')\n",
    "rdd1=sc.textFile(\"age.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD转换（年龄，1）形式的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda line : line.split(\" \")).map(lambda x:(int(x[1]),1))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.keys().reduce(lambda x,y : x+y)/rdd2.values().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"spark\",2),(\"hadoop\",6),(\"hadoop\",4),(\"spark\",6)])\n",
    "rdd.mapValues(lambda x : (x,1)).reduceByKey(lambda x,y : (x[0]+y[0],x[1] + y[1])).mapValues(lambda x : (x[0] / x[1])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([\"b\",\"a\",\"c\"])\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()\n",
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验四---SparkDataFram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "book.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "将book的DataFrame转化RDD操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "book_rdd.take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "创建dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df=spark.createDataFrame(book_rdd)\n",
    "book_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#指定列输出\n",
    "book_df.select('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#查看方法和属性\n",
    "dir(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "print(book_pds.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_pds=book_pds.toPandas()\n",
    "print(book_pds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "#转化Rdd\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],rul=x[5]))\n",
    "#创建dataFrame\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "# book_df.select(book_df['id'],book_df['name'],book_df['rating']-1).show()\n",
    "#pandas\n",
    "book_pds=book_df.groupBy('publish').count()\n",
    "book_pds=book_pds.toPandas()\n",
    "ax=book_pds.head(n=10).plot(x='publish',y=['count'],kind='bar',title='book count of publish')\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.appName(\"rddDataFrame\").master(\"local[*]\").getOrCreate()\n",
    "#读取数据\n",
    "book=spark.read.text('file:///opt/text/python/pyspark/book.txt')\n",
    "\n",
    "#查看前五行数据\n",
    "# book.take(5)\n",
    "\n",
    "#查看类型\n",
    "# type(book)\n",
    "\n",
    "#打印表头数据\n",
    "# book.printSchema()\n",
    "\n",
    "#将book的DataFrame转化为RRD\n",
    "book_rdd=book.rdd.map(lambda x:x[0].split(\",\")).map(lambda  x:Row(id=x[0],name=x[1],rating=x[2],price=x[3],publish=x[4],url=x[5]))\n",
    "# book_rdd.take(5)\n",
    "#创建dataFrame\n",
    "book_df=spark.createDataFrame(book_rdd)\n",
    "#查询前五行数据\n",
    "# book_df.show(5)\n",
    "#查询前50行\n",
    "#打印schema\n",
    "# book_df.printSchema()\n",
    "#创建临时表 \n",
    "book_df.registerTempTable(\"tb_book\")\n",
    "#使用sql语句查询 \n",
    "spark.sql(\"select * from tb_book\").show(15)\n",
    "#模糊查询书名包含“微积分”的书 \n",
    "spark.sql(\"select * from tb_book where name like '%微积分%'\").show()\n",
    "#输出图书的前10行的name和price字段信息\n",
    "spark.sql(\"select name,price from tb_book \").show(10)\n",
    "#统计书名包含“微积分”的书的数量 \n",
    "spark.sql(\"select count(*) from tb_book where name like '%微积分%'\").show()\n",
    "#查询评分大于9的图书 \n",
    "spark.sql('select * from tb_book where rating>9').show(10)\n",
    "#计算所有书名包含“微积分”的评分平均值 \n",
    "spark.sql(\"select avg(rating) rating_avg from tb_book where name like '%微积 分%'\")\n",
    "#把书目按照评分从高到低进行排列 \n",
    "spark.sql(\"select id,name,rating from tb_book order by rating desc\").show(15)\n",
    "#把图书按照出版社进行分组 \n",
    "spark.sql(\"select publish,count(name) as cnt from tb_book group by publish order by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验六"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# sc=SparkContext(\"local\",\"appName\")\n",
    "sc=SparkContext(\"local\",\"PythonStreamingHdfSWordCount\")\n",
    "scc=StreamingContext(sc,5)\n",
    "\n",
    "type(scc)\n",
    "\n",
    "# lines=scc.textFileStream(\"file:///opt/text/python/listen\")\n",
    "# lines=scc.textFileStream(\"hdfs://localhost:8020/listen\")\n",
    "lines=scc.socketTextStream('localhost',9999)\n",
    "\n",
    "\n",
    "\n",
    "counts = lines.flatMap(lambda line:line.split(\" \")).map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "counts.pprint()\n",
    "scc.start();scc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验七"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f9f7e30f619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#读取用户\n",
    "user_df=spark.read.text('file:///opt/text/python/ml-100k/u.user')\n",
    "user_df.show(10)\n",
    "\n",
    "#为用户数据添加 schem\n",
    "from pyspark import Row\n",
    "user_rdd = user_df.rdd.map(lambda x:x[0].split('|')).map(lambda x:Row(id=x[0],age=x[1],sex=x[2],job=x[3],code=x[4]))\n",
    "user_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建用户 dataframe\n",
    "user_df=spark.createDataFrame(user_rdd)\n",
    "\n",
    "#注册临时表\n",
    "user_df.registerTempTable(\"user\")\n",
    "spark.sql('select id,age,sex,job,code from user').show()\n",
    "\n",
    "\n",
    "#读取评分数据\n",
    "rating_df = spark.read.text('file:///opt/text/python/ml-100k/u.data')\n",
    "rating_df.show(5)\n",
    "\n",
    "\n",
    "#为评分数据添加 schema\n",
    "rating_rdd = rating_df.rdd.map(lambda x:x[0].split()).\\\n",
    "    map(lambda x: Row(user_id=x[0], film_id=x[1], rating=x[2], time=x[3]))\n",
    "rating_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建评分 dataframe\n",
    "rating_df = spark.createDataFrame(rating_rdd)\n",
    "\n",
    "#注册评分临时表\n",
    "rating_df.registerTempTable('rating')\n",
    "spark.sql(\"select user_id,film_id,rating,time from rating\").show()\n",
    "\n",
    "\n",
    "#读取电影信息\n",
    "film_df = spark.read.text('file:///opt/text/python/ml-100k/u.item')\n",
    "film_df.show()\n",
    "\n",
    "\n",
    "#为电影数据添加 schema\n",
    "film_rdd = film_df.rdd.map(lambda x:x[0].split('|')).map(lambda x:Row(id=x[0],title=x[1],post_time=x[2],url=x[4]))\n",
    "film_rdd.take(5)\n",
    "\n",
    "\n",
    "#创建电影 dataframe\n",
    "film_df = spark.createDataFrame(film_rdd)\n",
    "#注册电影临时表\n",
    "film_df.registerTempTable('film')\n",
    "spark.sql('select * from film').show()\n",
    "\n",
    "#导入 Spark 机器学习包：协同过滤算法\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "#ALS.trainImplicit 中第一个参数 RDD 是固定格式（用户编号，电影编号，评分值）\n",
    "rdd = rating_rdd.map(lambda x: (x[3], x[0], x[1]))\n",
    "rdd.take(5)\n",
    "\n",
    "\n",
    "#训练模型\n",
    "model = ALS.train(rdd, 10, 10, 0.01)\n",
    "#使用模型进行推荐，参数为用户 id，推荐个数\n",
    "user_film = model.recommendProducts(112, 3)\n",
    "print(user_film)\n",
    "\n",
    "#显示推荐电影名称\n",
    "type(user_film)\n",
    "for film in user_film:\n",
    "    spark.sql(\"select id ,title,post_time,url , \"+str(film[2])+\" as similar from film where id=\"+str(film[1])).show()\n",
    "\n",
    "\n",
    "#查看用户看过的电影\n",
    "spark.sql(\"select a.user_id,b.title from rating a,film b where a.film_id=b.id and a.user_id=112\").show()\n",
    "\n",
    "\n",
    "#保存模型\n",
    "save_path = '/opt/text/python/film_model'\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "model.save(sc, save_path)\n",
    "print('model saved!')\n",
    "#调用模型\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "model = MatrixFactorizationModel.load(sc, save_path)\n",
    "result = model.recommendProducts(112, 3)\n",
    "for film in user_film:\n",
    "    spark.sql(\"select id , title,post_time,url , \"+str(film[2])+\" as similar from film where id=\"+str(film[1])).show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验八 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
